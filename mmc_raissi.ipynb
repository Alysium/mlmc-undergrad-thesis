{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Sine(nn.Module):\n",
    "    \"\"\"This class defines the Sine activation function as a nn.Module\"\"\"\n",
    "    def __init__(self):\n",
    "        super(Sine, self).__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.sin(x)\n",
    "    \n",
    "    \n",
    "class LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, layers, stable, activation):\n",
    "        super(LSTM, self).__init__()\n",
    "\n",
    "        self.Wi = nn.Linear(in_features=layers[0], out_features=layers[1])\n",
    "\n",
    "        self.Wf = nn.Linear(in_features=layers[0], out_features=layers[1])\n",
    "\n",
    "        self.Wg = nn.Linear(in_features=layers[0], out_features=layers[1])\n",
    "\n",
    "        self.Wo = nn.Linear(in_features=layers[0], out_features=layers[1])\n",
    "        \n",
    "        self.Wout = nn.Linear(in_features=layers[1], out_features=layers[2])\n",
    "        \n",
    "        self.activation = activation\n",
    "\n",
    "        self.epsilon = 0.01\n",
    "        self.stable = stable\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        i = nn.Sigmoid()(self.Wi(x))\n",
    "        f = nn.Sigmoid()(self.Wf(x))\n",
    "        g = nn.Tanh()(self.Wg(x))\n",
    "        o = nn.Sigmoid()(self.Wo(x))\n",
    "        c_new = i*g\n",
    "        h_new = o*nn.Tanh()(c_new)\n",
    "        out = self.Wout(h_new)\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class Resnet(nn.Module):\n",
    "\n",
    "    def __init__(self, layers, stable, activation):\n",
    "        super(Resnet, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(in_features=layers[0], out_features=layers[1])\n",
    "        self.layer2 = nn.Linear(in_features=layers[1], out_features=layers[2])\n",
    "        self.layer2_input = nn.Linear(in_features=layers[0], out_features=layers[2])\n",
    "        self.layer3 = nn.Linear(in_features=layers[2], out_features=layers[3])\n",
    "        self.layer3_input = nn.Linear(in_features=layers[0], out_features=layers[3])\n",
    "        self.layer4 = nn.Linear(in_features=layers[3], out_features=layers[4])\n",
    "        self.layer4_input = nn.Linear(in_features=layers[0], out_features=layers[4])\n",
    "        self.layer5 = nn.Linear(in_features=layers[4], out_features=layers[5])\n",
    "\n",
    "        self.activation = activation\n",
    "\n",
    "        self.epsilon = 0.01\n",
    "        self.stable = stable\n",
    "\n",
    "    def stable_forward(self, layer, out):  # Building block for the NAIS-Net\n",
    "        weights = layer.weight\n",
    "        delta = 1 - 2 * self.epsilon\n",
    "        RtR = torch.matmul(weights.t(), weights)\n",
    "        norm = torch.norm(RtR)\n",
    "        if norm > delta:\n",
    "            RtR = delta ** (1 / 2) * RtR / (norm ** (1 / 2))\n",
    "        A = RtR + torch.eye(RtR.shape[0]).cuda() * self.epsilon\n",
    "\n",
    "        return F.linear(out, -A, layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        u = x\n",
    "\n",
    "        out = self.layer1(x)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        shortcut = out\n",
    "        if self.stable:\n",
    "            out = self.stable_forward(self.layer2, out)\n",
    "            out += self.layer2_input(u)\n",
    "        else:\n",
    "            out = self.layer2(out)\n",
    "        out = self.activation(out)\n",
    "        out += shortcut\n",
    "\n",
    "        shortcut = out\n",
    "        if self.stable:\n",
    "            out = self.stable_forward(self.layer3, out)\n",
    "            out += self.layer3_input(u)\n",
    "        else:\n",
    "            out = self.layer3(out)\n",
    "        out = self.activation(out)\n",
    "        out += shortcut\n",
    "\n",
    "        shortcut = out\n",
    "        if self.stable:\n",
    "            out = self.stable_forward(self.layer4, out)\n",
    "            out += self.layer4_input(u)\n",
    "        else:\n",
    "            out = self.layer4(out)\n",
    "\n",
    "        out = self.activation(out)\n",
    "        out += shortcut\n",
    "\n",
    "        out = self.layer5(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class SDEnet(nn.Module):\n",
    "\n",
    "    def __init__(self, layers, activation):\n",
    "        super(SDEnet, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.brownian = nn.ModuleList()\n",
    "\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(nn.Linear(in_features=layers[i], out_features=layers[i + 1]))\n",
    "            if i > 0 and i < len(layers) - 2:\n",
    "                self.brownian.append(nn.Linear(in_features=layers[i], out_features=1, bias=False))\n",
    "\n",
    "        self.activation = activation\n",
    "        self.epsilon = 1e-4\n",
    "        self.h = 0.1\n",
    "\n",
    "    def product(self, layer, out):\n",
    "        weights = layer.weight\n",
    "        RtR = torch.matmul(weights.t(), weights)\n",
    "        A = RtR + torch.eye(RtR.shape[0]).cuda() * self.epsilon\n",
    "\n",
    "        return F.linear(out, A, layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.layers[0](x)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        for i, layer in enumerate(self.layers[1:-1]):\n",
    "            shortcut = out\n",
    "            out = layer(out)\n",
    "            out = shortcut + self.h * self.activation(out) + self.h ** (1 / 2) * self.product(self.brownian[i],\n",
    "                                                                                              torch.rand_like(out))\n",
    "            # out = shortcut + self.activation(out) + 0.4*torch.ones_like(out)*torch.rand_like(out)\n",
    "\n",
    "        out = self.layers[-1](out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class VerletNet(nn.Module):\n",
    "\n",
    "    def __init__(self, layers, activation):\n",
    "        super(VerletNet, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(nn.Linear(in_features=layers[i], out_features=layers[i + 1]))\n",
    "\n",
    "        self.h = 0.5\n",
    "        self.activation = activation\n",
    "\n",
    "    def transpose(self, layer, out):\n",
    "\n",
    "        return F.linear(out, layer.weight.t(), layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.layers[0](x)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        z = torch.zeros_like(out)\n",
    "\n",
    "        for layer in self.layers[1:-1]:\n",
    "            shortcut = out\n",
    "            out = self.transpose(layer, out)\n",
    "            z = z - self.activation(out)\n",
    "            out = layer(z)\n",
    "            out = shortcut + self.activation(out)\n",
    "\n",
    "        out = self.layers[-1](out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class VerletNet(nn.Module):\n",
    "\n",
    "    def __init__(self, layers, activation):\n",
    "        super(VerletNet, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        for i in range(len(layers) - 1):\n",
    "            self.layers.append(nn.Linear(in_features=layers[i], out_features=layers[i + 1]))\n",
    "\n",
    "        self.h = 0.5\n",
    "        self.activation = activation\n",
    "\n",
    "    def transpose(self, layer, out):\n",
    "        return F.linear(out, layer.weight.t(), layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        out = self.layers[0](x)\n",
    "        out = self.activation(out)\n",
    "\n",
    "        z = torch.zeros_like(out)\n",
    "\n",
    "        for layer in self.layers[1:-1]:\n",
    "            shortcut = out\n",
    "            out = self.transpose(layer, out)\n",
    "            z = z - self.activation(out)\n",
    "            out = layer(z)\n",
    "            out = shortcut + self.activation(out)\n",
    "\n",
    "        out = self.layers[-1](out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from abc import ABC, abstractmethod\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "#from Models import Resnet, Sine\n",
    "\n",
    "\n",
    "class FBSNN(ABC):\n",
    "    def __init__(self, Xi, T, M, N, D, layers, mode, activation):\n",
    "        device_idx = 0\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.device(\"cuda:\" + str(device_idx) if torch.cuda.is_available() else \"cpu\")\n",
    "            torch.backends.cudnn.deterministic = True\n",
    "        else:\n",
    "            self.device = torch.device(\"cpu\")\n",
    "\n",
    "        #  We set a random seed to ensure that your results are reproducible\n",
    "        # torch.manual_seed(0)\n",
    "\n",
    "        self.Xi = torch.from_numpy(Xi).float().to(self.device)  # initial point\n",
    "        self.Xi.requires_grad = True\n",
    "\n",
    "        self.T = T  # terminal time\n",
    "        self.M = M  # number of trajectories\n",
    "        self.N = N  # number of time snapshots\n",
    "        self.D = D  # number of dimensions\n",
    "        self.mode = mode  # architecture: FC, Resnet and NAIS-Net are available\n",
    "        self.activation = activation\n",
    "        if activation == \"Sine\":\n",
    "            self.activation_function = Sine()\n",
    "        elif activation == \"ReLU\":\n",
    "            self.activation_function = nn.ReLU()\n",
    "\n",
    "        # initialize NN\n",
    "        if self.mode == \"FC\":\n",
    "            self.layers = []\n",
    "            for i in range(len(layers) - 2):\n",
    "                self.layers.append(nn.Linear(in_features=layers[i], out_features=layers[i + 1]))\n",
    "                self.layers.append(self.activation_function)\n",
    "            self.layers.append(nn.Linear(in_features=layers[-2], out_features=layers[-1]))\n",
    "\n",
    "            self.model = nn.Sequential(*self.layers).to(self.device)\n",
    "\n",
    "        elif self.mode == \"NAIS-Net\":\n",
    "            self.model = Resnet(layers, stable=True, activation=self.activation_function).to(self.device)\n",
    "        elif self.mode == \"Resnet\":\n",
    "            self.model = Resnet(layers, stable=False, activation=self.activation_function).to(self.device)\n",
    "        elif self.mode == \"Verlet\":\n",
    "            self.model = VerletNet(layers, activation=self.activation_function).to(self.device)\n",
    "        elif self.mode == \"SDEnet\":\n",
    "            self.model = SDEnet(layers, activation=self.activation_function).to(self.device)\n",
    "        elif self.mode == \"LSTM\":\n",
    "            self.model = LSTM(layers, stable=True, activation=self.activation_function).to(self.device)\n",
    "\n",
    "        self.model.apply(self.weights_init)\n",
    "        #print(self.model)\n",
    "\n",
    "        # Record the loss\n",
    "\n",
    "        self.training_loss = []\n",
    "        self.iteration = []\n",
    "\n",
    "    def weights_init(self, m):\n",
    "        if type(m) == nn.Linear:\n",
    "            torch.nn.init.xavier_uniform_(m.weight)\n",
    "\n",
    "    #calculates next Yt and Zt value\n",
    "    # also calculates initail Y0 and Z0 calue\n",
    "    def net_u(self, t, X):  # M x 1, M x D\n",
    "\n",
    "        input = torch.cat((t, X), 1)\n",
    "        u = self.model(input)  # M x 1\n",
    "        Du = torch.autograd.grad(outputs=[u], inputs=[X], grad_outputs=torch.ones_like(u), allow_unused=True,\n",
    "                                 retain_graph=True, create_graph=True)[0]\n",
    "        return u, Du\n",
    "\n",
    "    def Dg_tf(self, X):  # M x D\n",
    "\n",
    "        g = self.g_tf(X)\n",
    "        Dg = torch.autograd.grad(outputs=[g], inputs=[X], grad_outputs=torch.ones_like(g), allow_unused=True,\n",
    "                                 retain_graph=True, create_graph=True)[0]  # M x D\n",
    "        return Dg\n",
    "\n",
    "    def loss_function(self, t, W, Xi):\n",
    "        loss = 0\n",
    "        X_list = []\n",
    "        Y_list = []\n",
    "\n",
    "        t0 = t[:, 0, :]\n",
    "        W0 = W[:, 0, :]\n",
    "\n",
    "        X0 = Xi.repeat(self.M, 1).view(self.M, self.D)  # M x D\n",
    "        #there are M separate samples of initial inputs\n",
    "        Y0, Z0 = self.net_u(t0, X0)  # M x 1, M x D\n",
    "        #M samples of outputs and change in outputs (Z)\n",
    "\n",
    "        X_list.append(X0)\n",
    "        Y_list.append(Y0)\n",
    "\n",
    "        size = t.shape[1] #size = number of timestamps N\n",
    "\n",
    "        for n in range(0, size-1): #iterates through the timestamps\n",
    "            # print(n)\n",
    "            # print(size)\n",
    "            # print(t)\n",
    "            #specifies the current timestamp\n",
    "            t1 = t[:, n + 1, :] \n",
    "            W1 = W[:, n + 1, :]\n",
    "            #calculating each time step in equation (7)\n",
    "            X1 = X0 + self.mu_tf(t0, X0, Y0, Z0) * (t1 - t0) + torch.squeeze(\n",
    "                torch.matmul(self.sigma_tf(t0, X0, Y0), (W1 - W0).unsqueeze(-1)), dim=-1)\n",
    "            Y1_tilde = Y0 + self.phi_tf(t0, X0, Y0, Z0) * (t1 - t0) + torch.sum(\n",
    "                Z0 * torch.squeeze(torch.matmul(self.sigma_tf(t0, X0, Y0), (W1 - W0).unsqueeze(-1))), dim=1,\n",
    "                keepdim=True)\n",
    "            #this gets the next values of Y1 based on the NN model\n",
    "            Y1, Z1 = self.net_u(t1, X1) \n",
    "\n",
    "            loss += torch.sum(torch.pow(Y1 - Y1_tilde, 2))\n",
    "            #Y1_tilde = Y^n_m + phi*delta_t + (Z^n_m)'*Sigma^n_m*deltaW  \n",
    "\n",
    "            t0 = t1\n",
    "            W0 = W1\n",
    "            X0 = X1 # M x D\n",
    "            Y0 = Y1 # M x 1\n",
    "            Z0 = Z1\n",
    "\n",
    "            X_list.append(X0)\n",
    "            Y_list.append(Y0)\n",
    "\n",
    "        #Y1 is the \"payoff\" for final time\n",
    "        #self.g_tf\n",
    "        loss += torch.sum(torch.pow(Y1 - self.g_tf(X1), 2))\n",
    "        loss += torch.sum(torch.pow(Z1 - self.Dg_tf(X1), 2)) # can remove this and perform training too\n",
    "        #now we have iterated over all the timestamps and have calcualted the losses and accumulated the X's and Y's used to caluclate the losses\n",
    "\n",
    "        #length of X_list and Y_list is N + 1 (number of timestamps + 1)\n",
    "        # + 1 is present due to beginning and ending times [0,T]\n",
    "        # ie: for N = 2 -> time steps are [0, T/2, T]\n",
    "        X = torch.stack(X_list, dim=1) #shape: M x N+1 x D\n",
    "        Y = torch.stack(Y_list, dim=1) #shape: M x N+1 x 1\n",
    "\n",
    "        #returns loss (scalar), X (vector of all the X's generated), Y (vector of all resulting Y's), Y0\n",
    "        return loss, X, Y, Y[0, 0, 0]\n",
    "\n",
    "    def fetch_minibatch_train(self, size):  # Generate time + a Brownian motion\n",
    "        T = self.T\n",
    "\n",
    "        M = self.M #number of samples that we are going to generate\n",
    "        N = size #number of timesteps that will be uised\n",
    "        D = self.D #number of dimensions\n",
    "\n",
    "        Dt = np.zeros((M, N + 1, 1))  # M x (N+1) x 1\n",
    "        DW = np.zeros((M, N + 1, D))  # M x (N+1) x D\n",
    "\n",
    "        dt = T / N\n",
    "\n",
    "        Dt[:, 1:, :] = dt\n",
    "        DW[:, 1:, :] = np.sqrt(dt) * np.random.normal(size=(M, N, D))\n",
    "\n",
    "        #cumsum = cumulative sum\n",
    "        t = np.cumsum(Dt, axis=1)  # M x (N+1) x 1\n",
    "        W = np.cumsum(DW, axis=1)  # M x (N+1) x D\n",
    "        t = torch.from_numpy(t).float().to(self.device)\n",
    "        W = torch.from_numpy(W).float().to(self.device)\n",
    "\n",
    "        return t, W\n",
    "\n",
    "    def fetch_minibatch(self):  # Generate time + a Brownian motion\n",
    "        T = self.T\n",
    "\n",
    "        M = self.M\n",
    "        N = self.N\n",
    "        D = self.D\n",
    "\n",
    "        Dt = np.zeros((M, N + 1, 1))  # M x (N+1) x 1\n",
    "        DW = np.zeros((M, N + 1, D))  # M x (N+1) x D\n",
    "\n",
    "        dt = T / N\n",
    "\n",
    "        Dt[:, 1:, :] = dt\n",
    "        DW[:, 1:, :] = np.sqrt(dt) * np.random.normal(size=(M, N, D))\n",
    "\n",
    "        t = np.cumsum(Dt, axis=1)  # M x (N+1) x 1\n",
    "        W = np.cumsum(DW, axis=1)  # M x (N+1) x D\n",
    "        t = torch.from_numpy(t).float().to(self.device)\n",
    "        W = torch.from_numpy(W).float().to(self.device)\n",
    "\n",
    "        return t, W\n",
    "\n",
    "    def train(self, N_Iter, learning_rate, L, h_Factor, fixed=0):\n",
    "        '''\n",
    "          L = number of layers that will be performed\n",
    "          h_Factor = factor in which each number of timesteps will be divided against\n",
    "        '''\n",
    "        loss_temp = np.array([])\n",
    "\n",
    "        previous_it = 0\n",
    "        if self.iteration != []:\n",
    "            previous_it = self.iteration[-1]\n",
    "          \n",
    "        #want to divide evently across the layers\n",
    "        layerIters = N_Iter // L\n",
    "\n",
    "        # Optimizers\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "\n",
    "        start_time = time.time()\n",
    "        time_per_epoch = []\n",
    "        \n",
    "        numLayers = []\n",
    "        for it in range(previous_it, previous_it + N_Iter):\n",
    "            #size = number of time steps between [0,T]\n",
    "            #this is the point where multilayer monte carlo is used------\n",
    "\n",
    "            if fixed==0:\n",
    "              l = it//layerIters+1 #layers 1 to L\n",
    "              size = h_Factor**l       \n",
    "              if numLayers == [] or l != numLayers[-1][0]:\n",
    "                numLayers.append([l,size])   \n",
    "            else:\n",
    "              size = fixed\n",
    "\n",
    "\n",
    "\n",
    "            # if it < N_Iter/5:\n",
    "            #   size = 2\n",
    "            # elif N_Iter/5 <= it < (2*N_Iter)/5:\n",
    "            #   size = 4\n",
    "            # elif (2*N_Iter)/5 <= it < (3*N_Iter)/5:\n",
    "            #   size = 8\n",
    "            # elif (3*N_Iter)/5 <= it < (4*N_Iter)/5:\n",
    "            #   size = 16\n",
    "            # else:\n",
    "            #   size = 32\n",
    "            #------------------------------------------------------------\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "\n",
    "            #this step fetches the samples of the number of timestamps that will be used\n",
    "            t_batch, W_batch = self.fetch_minibatch_train(size)  # M x (N+1) x 1, M x (N+1) x D\n",
    "            #---------------------------------------------------------------------------\n",
    "            '''\n",
    "            The vectors are as:\n",
    "            [M, N+1, 1] or [M, N+1, D]\n",
    "            -> index 0: number of samples that will be run\n",
    "            -> index 1: number of timesteps run for each sample\n",
    "            -> index 2: number of dimensions\n",
    "              -> time only has 1 dimensions\n",
    "              -> Brownian motion has 5 dimensions for 5 dimensional input\n",
    "\n",
    "            '''\n",
    "\n",
    "            loss, X_pred, Y_pred, Y0_pred = self.loss_function(t_batch, W_batch, self.Xi)\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            loss_temp = np.append(loss_temp, loss.cpu().detach().numpy())\n",
    "\n",
    "            # Print\n",
    "            \n",
    "            if it % 100 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                time_per_epoch.append(elapsed)\n",
    "                print('It: %d, Loss: %.3e, Y0: %.3f, Time: %.2f, Learning Rate: %.3e' %\n",
    "                      (it, loss, Y0_pred, elapsed, learning_rate))\n",
    "                start_time = time.time()\n",
    "\n",
    "\n",
    "            # Loss\n",
    "            if it % 100 == 0:\n",
    "                self.training_loss.append(loss_temp.mean())\n",
    "                loss_temp = np.array([])\n",
    "\n",
    "                self.iteration.append(it)\n",
    "\n",
    "            graph = np.stack((self.iteration, self.training_loss))\n",
    "        plt.plot(list(np.linspace(0,N_Iter-100,int(N_Iter/100))), time_per_epoch)\n",
    "        plt.xlabel(\"epoch\")\n",
    "        plt.ylabel(\"time(seconds)\")\n",
    "        plt.title(\"Time in seconds per epoch\")\n",
    "        print(\"Number of layers arr\", numLayers)\n",
    "        return graph\n",
    "\n",
    "    def predict(self, Xi_star, t_star, W_star):\n",
    "        Xi_star = torch.from_numpy(Xi_star).float().to(self.device)\n",
    "        Xi_star.requires_grad = True\n",
    "        loss, X_star, Y_star, Y0_pred = self.loss_function(t_star, W_star, Xi_star)\n",
    "\n",
    "        return X_star, Y_star\n",
    "\n",
    "    ###########################################################################\n",
    "    ############################# Change Here! ################################\n",
    "    ###########################################################################\n",
    "    @abstractmethod\n",
    "    def phi_tf(self, t, X, Y, Z):  # M x 1, M x D, M x 1, M x D\n",
    "        pass  # M x1\n",
    "\n",
    "    @abstractmethod\n",
    "    def g_tf(self, X):  # M x D\n",
    "        pass  # M x 1\n",
    "\n",
    "    @abstractmethod\n",
    "    def mu_tf(self, t, X, Y, Z):  # M x 1, M x D, M x 1, M x D\n",
    "        M = self.M\n",
    "        D = self.D\n",
    "        return torch.zeros([M, D]).to(self.device)  # M x D\n",
    "\n",
    "    @abstractmethod\n",
    "    def sigma_tf(self, t, X, Y):  # M x 1, M x D, M x 1\n",
    "        M = self.M\n",
    "        D = self.D\n",
    "        return torch.diag_embed(torch.ones([M, D])).to(self.device)  # M x D x D\n",
    "    ###########################################################################\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "#from FBSNNs import FBSNN\n",
    "\n",
    "\"\"\"\n",
    "t1 = t[:, n + 1, :]\n",
    "W1 = W[:, n + 1, :]\n",
    "#calculating each time step in equation (7)\n",
    "X1 = X0 + self.mu_tf(t0, X0, Y0, Z0) * (t1 - t0) + torch.squeeze(\n",
    "    torch.matmul(self.sigma_tf(t0, X0, Y0), (W1 - W0).unsqueeze(-1)), dim=-1)\n",
    "Y1_tilde = Y0 + self.phi_tf(t0, X0, Y0, Z0) * (t1 - t0) + torch.sum(\n",
    "    Z0 * torch.squeeze(torch.matmul(self.sigma_tf(t0, X0, Y0), (W1 - W0).unsqueeze(-1))), dim=1,\n",
    "    keepdim=True)\n",
    "Y1, Z1 = self.net_u(t1, X1)\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "class BlackScholesBarenblatt(FBSNN):\n",
    "    def __init__(self, Xi, T, M, N, D, layers, mode, activation):\n",
    "\n",
    "        super().__init__(Xi, T, M, N, D, layers, mode, activation)\n",
    "\n",
    "    #used to calculate Y_{n+1}\n",
    "    def phi_tf(self, t, X, Y, Z):  # M x 1, M x D, M x 1, M x D\n",
    "        interestRate = 0.05\n",
    "        return interestRate * (Y - torch.sum(X * Z, dim=1, keepdim=True))  # M x 1\n",
    "\n",
    "    #used to calculate the terminal value\n",
    "    def g_tf(self, X):  # M x D\n",
    "        return torch.sum(X ** 2, 1, keepdim=True)  # M x 1 \n",
    "        # g(s) = |x|^2\n",
    "\n",
    "    #for BSB eqn, mu_tf = 0\n",
    "    def mu_tf(self, t, X, Y, Z):  # M x 1, M x D, M x 1, M x D\n",
    "        return super().mu_tf(t, X, Y, Z)  # M x D\n",
    "\n",
    "    #used to calculate X_{n+1} and Y_{n+1}\n",
    "    def sigma_tf(self, t, X, Y):  # M x 1, M x D, M x 1\n",
    "        sigma = 0.4\n",
    "        return sigma * torch.diag_embed(X)  # M x D x D\n",
    "\n",
    "    ###########################################################################\n",
    "\n",
    "\n",
    "#equation (16), used to calculate loss\n",
    "# only used for to generate test graph (plotting 100-dimensional B-S-B eq) where comparing learned with exact\n",
    "def u_exact(t, X):  # (N+1) x 1, (N+1) x D\n",
    "    r = 0.05\n",
    "    sigma_max = 0.4\n",
    "    return np.exp((r + sigma_max ** 2) * (T - t)) * np.sum(X ** 2, 1, keepdims=True)  # (N+1) x 1\n",
    "\n",
    "\n",
    "def run_model(model, N_Iter, learning_rate, L, h_Factor, fixed=0):\n",
    "    tot = time.time()\n",
    "    samples = 5\n",
    "    print(model.device)\n",
    "    graph = model.train(N_Iter, learning_rate, L, h_Factor, fixed=fixed)\n",
    "    print(\"total time:\", time.time() - tot, \"s\")\n",
    "\n",
    "     \n",
    "    np.random.seed(42)\n",
    "    t_test, W_test = model.fetch_minibatch()\n",
    "    X_pred, Y_pred = model.predict(Xi, t_test, W_test)\n",
    "\n",
    "\n",
    "    if type(t_test).__module__ != 'numpy':\n",
    "        t_test = t_test.cpu().numpy()\n",
    "    if type(X_pred).__module__ != 'numpy':\n",
    "        X_pred = X_pred.cpu().detach().numpy()\n",
    "    if type(Y_pred).__module__ != 'numpy':\n",
    "        Y_pred = Y_pred.cpu().detach().numpy()\n",
    "\n",
    "    Y_test = np.reshape(u_exact(np.reshape(t_test[0:M, :, :], [-1, 1]), np.reshape(X_pred[0:M, :, :], [-1, D])),\n",
    "                        [M, -1, 1])\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(graph[0], graph[1])\n",
    "    plt.xlabel('Iterations')\n",
    "    plt.ylabel('Value')\n",
    "    plt.yscale(\"log\")\n",
    "    plt.title('Evolution of the training loss')\n",
    "    # plt.title('Evolution of the training loss')\n",
    "    plt.savefig(str(D) + '-dimensional Black-Scholes-Barenblatt loss, ' + model.mode + \"-\" + model.activation +\"-iters:\"+str(N_Iter)+\"-L:\"+str(L)+\",hFactor:\"+str(h_Factor) + \".pdf\", bbox_inches='tight')\n",
    "\n",
    "\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(t_test[0:1, :, 0].T, Y_pred[0:1, :, 0].T, 'b', label='Learned $u(t,X_t)$')\n",
    "    plt.plot(t_test[0:1, :, 0].T, Y_test[0:1, :, 0].T, 'r--', label='Exact $u(t,X_t)$')\n",
    "    plt.plot(t_test[0:1, -1, 0], Y_test[0:1, -1, 0], 'ko', label='$Y_T = u(T,X_T)$')\n",
    "\n",
    "    plt.plot(t_test[1:samples, :, 0].T, Y_pred[1:samples, :, 0].T, 'b')\n",
    "    plt.plot(t_test[1:samples, :, 0].T, Y_test[1:samples, :, 0].T, 'r--')\n",
    "    plt.plot(t_test[1:samples, -1, 0], Y_test[1:samples, -1, 0], 'ko')\n",
    "\n",
    "    plt.plot([0], Y_test[0, 0, 0], 'ks', label='$Y_0 = u(0,X_0)$')\n",
    "\n",
    "    plt.xlabel('$t$')\n",
    "    plt.ylabel('$Y_t = u(t,X_t)$')\n",
    "    plt.title(str(D) + '-dimensional Black-Scholes-Barenblatt paths, ' + model.mode + \"-\" + model.activation+\"-iters:\"+str(N_Iter)+\"-L:\"+str(L)+\",hFactor:\"+str(h_Factor))\n",
    "    # plt.legend()\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1.02))\n",
    "    plt.savefig(str(D) + '-dimensional Black-Scholes-Barenblatt paths, ' + model.mode + \"-\" + model.activation +\"-iters:\"+str(N_Iter)+\"-L:\"+str(L)+\",hFactor:\"+str(h_Factor) + \".pdf\", bbox_inches='tight')\n",
    "\n",
    "    errors = np.sqrt((Y_test - Y_pred) ** 2 / Y_test ** 2)\n",
    "    mean_errors = np.mean(errors, 0)\n",
    "    std_errors = np.std(errors, 0)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(t_test[0, :, 0], mean_errors, 'b', label='mean')\n",
    "    plt.plot(t_test[0, :, 0], mean_errors + 2 * std_errors, 'r--', label='mean + two standard deviations')\n",
    "    plt.xlabel('$t$')\n",
    "    plt.ylabel('relative error')\n",
    "    plt.title(str(D) + '-dimensional Black-Scholes-Barenblatt, ' + model.mode + \"-\" + model.activation +\"-iters:\"+str(N_Iter)+\"-L:\"+str(L)+\",hFactor:\"+str(h_Factor))\n",
    "    # plt.legend()\n",
    "    # plt.savefig(str(D) + '-dimensional Black-Scholes-Barenblatt, ' + model.mode + \"-\" + model.activation)\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1.02))\n",
    "    plt.savefig(str(D) + '-dimensional Black-Scholes-Barenblatt, ' + model.mode + \"-\" + model.activation +\"-iters:\"+str(N_Iter)+\"-L:\"+str(L)+\",hFactor:\"+str(h_Factor)+ \".pdf\", bbox_inches='tight')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    tot = time.time()\n",
    "    M = 10  # number of trajectories (batch size)\n",
    "      #in the paper, M = 100\n",
    "    N = 50  # number of time snapshots\n",
    "    D = 10  # number of dimensions\n",
    "\n",
    "    layers = [D + 1] + 4 * [256] + [1] #represents the number of neurons for each layer\n",
    "    #there are 4 hidden layers of 256 neurons\n",
    "    #layers = [D + 1] + [256] +[1]\n",
    "\n",
    "    Xi = np.array([1.0, 0.5] * int(D / 2))[None, :] #initaliztion of X0, sahpe (1,10)\n",
    "    T = 1.0 #total time\n",
    "    \"Available architectures\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    mode = \"FC\"\n",
    "    activation = \"ReLU\"  # Sine and ReLU are available\n",
    "    learning_rate = 1e-3\n",
    "    iterations = 2*10**3\n",
    "    model = BlackScholesBarenblatt(Xi, T,\n",
    "                                   M, N, D,\n",
    "                                   layers, mode, activation)\n",
    "    L = 5\n",
    "    h_Factor = 2\n",
    "    run_model(model, iterations, learning_rate, L, h_Factor)\n",
    "    #the number of iterations and learning rate are the same as the Raissi Paper for the first go around"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode = \"FC\"\n",
    "activation = \"ReLU\"  # Sine and ReLU are available\n",
    "learning_rate = 1e-3\n",
    "iterations = 2*10**3\n",
    "model = BlackScholesBarenblatt(Xi, T,\n",
    "                                M, N, D,\n",
    "                                layers, mode, activation)\n",
    "L = 1 #are not used since fixed is set\n",
    "h_factor=1 #are not used since fixed is set\n",
    "run_model(model, iterations, learning_rate, L,h_factor,fixed=32)\n",
    "#the number of iterations and learning rate are the same as the Raissi Paper for the first go around"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
